# ETL de Entregas de Productos con PySpark y OmegaConf

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://python.org)
[![PySpark](https://img.shields.io/badge/PySpark-3.4.0-orange)](https://spark.apache.org)
[![OmegaConf](https://img.shields.io/badge/OmegaConf-2.3.0-green)](https://omegaconf.readthedocs.io)

Sistema ETL robusto y parametrizable para el procesamiento de datos de entregas de productos, implementado con PySpark y configuraciÃ³n flexible usando OmegaConf.

## ğŸš€ CaracterÃ­sticas Principales

- **ConfiguraciÃ³n Flexible**: Uso de OmegaConf con archivos YAML para configuraciones parametrizables
- **Procesamiento Escalable**: Implementado con PySpark para manejo eficiente de grandes volÃºmenes de datos
- **Filtrado Avanzado**: Filtros configurables por fecha, paÃ­s y tipo de entrega
- **Calidad de Datos**: DetecciÃ³n y limpieza automÃ¡tica de anomalÃ­as
- **Transformaciones Inteligentes**: NormalizaciÃ³n de unidades y categorizaciÃ³n de entregas
- **Reportes de Calidad**: GeneraciÃ³n automÃ¡tica de mÃ©tricas y reportes de calidad
- **CLI Intuitiva**: Interfaz de lÃ­nea de comandos con mÃºltiples opciones

## ğŸ“‹ Requisitos del Sistema

- Python 3.8 o superior
- Java 8 o superior (requerido por Spark)
- MÃ­nimo 4GB de RAM
- 2GB de espacio libre en disco

## ğŸ› ï¸ InstalaciÃ³n

### 1. Clonar el Repositorio

```bash
git clone https://github.com/Danzstorm/etl-pyspark-project.git
cd etl-pyspark-project
```

### 2. Crear Entorno Virtual

```bash
python -m venv venv

# En Linux/macOS
source venv/bin/activate

# En Windows
venv\Scripts\activate
```

### 3. Instalar Dependencias

```bash
pip install -r requirements.txt
```

### 4. Configurar Estructura del Proyecto

```bash
python -c "from src.utils import setup_project_directories; setup_project_directories()"
```

### 5. Preparar Datos

Coloca tu archivo CSV en `data/raw/entregas_productos_prueba.csv`

## ğŸ¯ Uso RÃ¡pido

### EjecuciÃ³n BÃ¡sica

```bash
python main.py run
```

### Filtrar por Fechas y PaÃ­s

```bash
python main.py run --start-date 2025-01-01 --end-date 2025-06-30 --country GT
```

### Procesamiento para MÃºltiples PaÃ­ses

```bash
python main.py run --countries GT,PE,EC --start-date 2025-03-01 --end-date 2025-03-31
```

### Dry Run (ValidaciÃ³n sin Guardar)

```bash
python main.py run --dry-run --verbose
```

## ğŸ“– Comandos Disponibles

### Ejecutar Procesamiento

```bash
python main.py run [OPCIONES]
```

**Opciones principales:**
- `--config, -c`: Archivo de configuraciÃ³n (default: config/base_config.yaml)
- `--start-date, -s`: Fecha de inicio (YYYY-MM-DD)
- `--end-date, -e`: Fecha de fin (YYYY-MM-DD)
- `--country`: PaÃ­s especÃ­fico (GT, PE, EC, SV, HN, JM)
- `--countries`: Lista de paÃ­ses separados por coma
- `--input-path`: Ruta personalizada del archivo de entrada
- `--output-path`: Directorio personalizado de salida
- `--dry-run`: Ejecutar sin guardar datos
- `--verbose, -v`: Logging detallado

### Validar ConfiguraciÃ³n

```bash
python main.py validate --config config/dev_config.yaml
```

### Crear Configuraciones de Ejemplo

```bash
python main.py create-configs --output-dir config
```

## ğŸ“ Estructura del Proyecto

```
etl-pyspark-project/
â”œâ”€â”€ README.md                 # Este archivo
â”œâ”€â”€ requirements.txt          # Dependencias Python
â”œâ”€â”€ main.py                  # Script principal CLI
â”œâ”€â”€ config/                  # Configuraciones
â”‚   â”œâ”€â”€ base_config.yaml     # ConfiguraciÃ³n base
â”‚   â”œâ”€â”€ dev_config.yaml      # ConfiguraciÃ³n desarrollo
â”‚   â”œâ”€â”€ qa_config.yaml       # ConfiguraciÃ³n QA
â”‚   â””â”€â”€ prod_config.yaml     # ConfiguraciÃ³n producciÃ³n
â”œâ”€â”€ src/                     # CÃ³digo fuente
â”‚   â”œâ”€â”€ config_manager.py    # Gestor de configuraciÃ³n
â”‚   â”œâ”€â”€ data_processor.py    # Procesador principal
â”‚   â””â”€â”€ utils.py            # Utilidades
â”œâ”€â”€ data/                   # Datos
â”‚   â”œâ”€â”€ raw/                # Datos sin procesar
â”‚   â””â”€â”€ processed/          # Datos procesados (particionados)
â”œâ”€â”€ logs/                   # Archivos de log
â”œâ”€â”€ tests/                  # Pruebas unitarias
â”œâ”€â”€ docs/                   # DocumentaciÃ³n
â””â”€â”€ notebooks/              # Jupyter notebooks de exploraciÃ³n
```

## âš™ï¸ ConfiguraciÃ³n

El proyecto usa OmegaConf para manejar configuraciones flexibles. La configuraciÃ³n base estÃ¡ en `config/base_config.yaml`.

### Configuraciones Principales

#### Filtros de Datos
```yaml
data:
  filters:
    start_date: "2025-01-01"
    end_date: "2025-12-31"
    countries: ["GT", "PE", "EC", "SV", "HN", "JM"]
```

#### Transformaciones
```yaml
transformations:
  unit_conversion:
    cs_to_units_multiplier: 20  # 1 CS = 20 unidades
  
  delivery_types:
    routine: ["ZPRE", "ZVE1"]     # Entregas de rutina
    bonus: ["Z04", "Z05"]         # Entregas con bonificaciones
    exclude: ["COBR"]             # Tipos excluidos
```

#### Limpieza de Datos
```yaml
transformations:
  data_cleaning:
    remove_empty_materials: true
    remove_zero_prices: true
    remove_negative_quantities: true
```

## ğŸ“Š Funcionalidades Implementadas

### âœ… Procesamiento de Datos

1. **Carga de Datos**: Lectura de archivos CSV con schema definido
2. **Filtrado Configurable**: Por fechas, paÃ­ses y tipos de entrega
3. **Particionado**: Por fecha_proceso para optimizaciÃ³n
4. **ValidaciÃ³n**: Schema y tipos de datos

### âœ… Transformaciones

1. **NormalizaciÃ³n de Unidades**: CS (cajas) â†’ unidades (CS * 20)
2. **CategorizaciÃ³n de Entregas**: Rutina vs BonificaciÃ³n
3. **Limpieza de Datos**: EliminaciÃ³n de registros anÃ³malos
4. **Columnas Calculadas**: Valor total, scores de calidad

### âœ… Calidad de Datos

1. **DetecciÃ³n de AnomalÃ­as**: Precios cero, cantidades negativas, materiales vacÃ­os
2. **Reportes de Calidad**: MÃ©tricas detalladas de procesamiento
3. **ValidaciÃ³n de Schema**: Tipos de datos y estructura
4. **Scores de Calidad**: Por registro individual

### âœ… ConfiguraciÃ³n y ParametrizaciÃ³n

1. **OmegaConf**: ConfiguraciÃ³n flexible con YAML
2. **Overrides**: ParÃ¡metros desde lÃ­nea de comandos
3. **MÃºltiples Entornos**: dev, qa, prod
4. **ValidaciÃ³n de ConfiguraciÃ³n**: AutomÃ¡tica con Pydantic

## ğŸ“ˆ Salidas del Procesamiento

### Datos Procesados
- **Formato**: Parquet (configurable a CSV)
- **Particionado**: Por fecha_proceso
- **UbicaciÃ³n**: `data/processed/fecha_proceso=YYYYMMDD/`

### Columnas en el Dataset Final

| Columna | DescripciÃ³n | Tipo |
|---------|-------------|------|
| `pais_codigo` | CÃ³digo del paÃ­s | string |
| `fecha_proceso` | Fecha de procesamiento | date |
| `codigo_transporte` | CÃ³digo de transporte | string |
| `codigo_ruta` | CÃ³digo de ruta | string |
| `tipo_entrega_codigo` | CÃ³digo de tipo de entrega | string |
| `codigo_material` | CÃ³digo del material | string |
| `precio_unitario` | Precio por unidad | double |
| `cantidad_unidades` | Cantidad normalizada en unidades | double |
| `unidad_estandar` | Unidad estÃ¡ndar (units) | string |
| `es_entrega_rutina` | Flag: es entrega de rutina | int |
| `es_entrega_bonificacion` | Flag: es entrega con bonificaciÃ³n | int |
| `categoria_entrega` | CategorÃ­a (RUTINA/BONIFICACION) | string |
| `valor_total_calculado` | Precio Ã— cantidad normalizada | double |
| `score_calidad_datos` | Score de calidad (0-5) | int |
| `timestamp_procesamiento` | Timestamp de procesamiento | timestamp |
| `anio_proceso` | AÃ±o de proceso | string |
| `mes_proceso` | Mes de proceso | string |

### Reportes de Calidad
- **Formato**: JSON
- **Contenido**: MÃ©tricas de calidad, estadÃ­sticas, problemas detectados
- **UbicaciÃ³n**: `data/processed/quality_report_YYYYMMDD_HHMMSS.json`

## ğŸ”§ Desarrollo

### Estructura de Desarrollo en VS Code

1. **Instalar Extensiones Recomendadas**:
   - Python
   - Jupyter
   - YAML

2. **Configurar Python Interpreter**:
   ```
   Ctrl+Shift+P â†’ Python: Select Interpreter â†’ ./venv/bin/python
   ```

3. **Configurar Settings.json**:
   ```json
   {
       "python.defaultInterpreterPath": "./venv/bin/python",
       "python.linting.enabled": true,
       "python.linting.pylintEnabled": true
   }
   ```

### Desarrollo Local

```bash
# Ejecutar en modo desarrollo
python main.py run --config config/dev_config.yaml --verbose

# Validar configuraciÃ³n
python main.py validate --config config/dev_config.yaml

# Ejecutar tests
python -m pytest tests/ -v

# Formatear cÃ³digo
black src/ main.py

# AnÃ¡lisis estÃ¡tico
flake8 src/ main.py
```

## ğŸš€ MigraciÃ³n a Databricks

### 1. Preparar Archivos

```bash
# Crear zip con el cÃ³digo fuente
zip -r etl_project.zip src/ config/ main.py requirements.txt
```

### 2. Subir a Databricks

1. **Workspace** â†’ **Import** â†’ Subir `etl_project.zip`
2. **Clusters** â†’ Crear cluster con Spark 3.4.0
3. **Libraries** â†’ Instalar desde PyPI: omegaconf, pydantic

### 3. Adaptar Rutas

```python
# En Databricks, cambiar rutas locales por DBFS
input_path = "/dbfs/FileStore/data/entregas_productos_prueba.csv"
output_path = "/dbfs/FileStore/processed/"
```

### 4. Ejecutar en Notebook

```python
%run ./main.py run --config /dbfs/FileStore/config/base_config.yaml
```

## ğŸ§ª Pruebas

```bash
# Ejecutar todas las pruebas
python -m pytest tests/ -v

# Ejecutar con cobertura
python -m pytest tests/ --cov=src --cov-report=html

# Prueba especÃ­fica
python -m pytest tests/test_data_processor.py::test_load_data -v
```

## ğŸ“š DocumentaciÃ³n Adicional

- [Flujo de Datos Detallado](docs/flujo_datos.md)
- [ConfiguraciÃ³n Avanzada](docs/configuracion.md)
- [GuÃ­a de Troubleshooting](docs/troubleshooting.md)
- [API Reference](docs/api_reference.md)

## ğŸ¤ ContribuciÃ³n

1. Fork el repositorio
2. Crear rama feature (`git checkout -b feature/nueva-funcionalidad`)
3. Commit cambios (`git commit -am 'Agregar nueva funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Crear Pull Request

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Ver [LICENSE](LICENSE) para mÃ¡s detalles.

## ğŸ†˜ Soporte

Para reportar bugs o solicitar nuevas funcionalidades, crear un [issue](https://github.com/tu-usuario/etl-pyspark-project/issues).

## ğŸ† CaracterÃ­sticas Destacadas

- âœ… **ConfiguraciÃ³n Flexible**: OmegaConf con YAML
- âœ… **Escalabilidad**: PySpark para big data
- âœ… **Calidad de Datos**: ValidaciÃ³n y limpieza automÃ¡tica
- âœ… **ParametrizaciÃ³n**: Filtros configurables
- âœ… **Monitoreo**: Reportes de calidad detallados
- âœ… **CLI Amigable**: Interfaz intuitiva
- âœ… **DocumentaciÃ³n Completa**: GuÃ­as y ejemplos
- âœ… **EstÃ¡ndares de CÃ³digo**: PEP8, type hints, docstrings
- âœ… **Testing**: Pruebas unitarias
- âœ… **Compatibilidad**: Local, Databricks, otros clusters

---
